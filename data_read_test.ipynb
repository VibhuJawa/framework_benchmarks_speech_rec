{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn as nn\n",
    "from torch import optim\n",
    "from torch import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dir = 'data/pickled_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2_num_mapping = {}\n",
    "num_2_label_ar= []\n",
    "data = {\"x\":[],\"y\":[]}\n",
    "\n",
    "label_num = 0\n",
    "\n",
    "for label in os.listdir(input_dir):\n",
    "    if label == '.DS_Store':\n",
    "        continue\n",
    "    p_file = input_dir + label\n",
    "    label_2_num_mapping[label]=label_num\n",
    "    num_2_label_ar.append(label)\n",
    "\n",
    "    with open(p_file, 'rb') as f:\n",
    "        label_examples  = pickle.load(f)\n",
    "        # data dim = num_examples * time * frequency\n",
    "        label_ar = [np.swapaxes(np.array(example),0,1) for example in label_examples]\n",
    "        data['x'].extend(label_ar)\n",
    "        y_labels = [label_num for i in range(0,len(label_examples))]\n",
    "        data['y'].extend(y_labels)\n",
    "   \n",
    "    label_num+=1\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(n, X,Y):\n",
    "    \"\"\"\n",
    "    takes input and returns padded sample\n",
    "    n= num_samples\n",
    "    X = input_featurs_list\n",
    "    Y = label_list\n",
    "    \"\"\"\n",
    "    label_ids = np.random.randint(low = 0,high = len(X), size=n)\n",
    "    frequency = len(X[0][0])\n",
    "    sampled_X = [X[label_id] for label_id in label_ids]\n",
    "    sampled_y = [Y[label_id] for label_id in label_ids]\n",
    "    padded_X = []\n",
    "    \n",
    "    max_batch_len = max([len(x) for x in sampled_X])\n",
    "    for x in sampled_X:\n",
    "        padding_time_count = max_batch_len-len(x)\n",
    "        if padding_time_count!=0:\n",
    "            x_padded = np.zeros(shape = (max_batch_len,frequency))\n",
    "            x_padded[:x.shape[0],:x.shape[1]] = x\n",
    "            padded_X.append(x_padded)\n",
    "        else:\n",
    "            padded_X.append(x)\n",
    "        \n",
    "    return np.asarray(padded_X),np.asarray(sampled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_samples ,y_samples = sample_batch(32,X = data['x'],Y=data['y'])\n",
    "# print(x_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_Predictor_RNN(nn.Module):\n",
    "    def __init__(self, input_freq=20,hidden_size=256,linear_output_size = 128, n_categories=30):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_freq, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(hidden_size, linear_output_size)\n",
    "        self.linear2 = nn.Linear(linear_output_size, n_categories)\n",
    "\n",
    "    def forward(self, x, initial_states):\n",
    "        # input now stores floats and has shape [length, batch_size, embedding_size]\n",
    "        self.rnn.flatten_parameters()\n",
    "        x, final_states = self.rnn(x, initial_states)  # TODO\n",
    "        x = final_states[0][0] * final_states[0][1]\n",
    "        self.rnn.flatten_parameters()\n",
    "        x = nn.functional.relu6(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model  Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_categories = len(label_2_num_mapping)\n",
    "batch_size = 1\n",
    "input_freq = 20\n",
    "hidden_size=256\n",
    "n_epochs = 10\n",
    "n_iters = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.3885512351989746\n",
      "2 3.403934955596924\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-08b770f20ac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/torch_3.1/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/torch_3.1/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Word_Predictor_RNN(input_freq=input_freq,hidden_size=hidden_size,n_categories=n_categories)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "losses = np.zeros(n_epochs) # For plotting\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for iter in range(n_iters):\n",
    "        _inputs,_labels = sample_batch(50,data['x'],data['y'])\n",
    "        inputs = torch.autograd.Variable(torch.from_numpy(_inputs).float())\n",
    "        targets = torch.autograd.Variable(torch.from_numpy(_labels))\n",
    "\n",
    "        outputs = model(inputs, None)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses[epoch] += loss.data[0]\n",
    "\n",
    "    if epoch > 0:\n",
    "        print(epoch, loss.data[0])\n",
    "\n",
    "    # Use some plotting library\n",
    "    # if epoch % 10 == 0:\n",
    "        # show_plot('inputs', _inputs, True)\n",
    "        # show_plot('outputs', outputs.data.view(-1), True)\n",
    "        # show_plot('losses', losses[:epoch] / n_iters)\n",
    "\n",
    "        # Generate a test\n",
    "        # outputs, hidden = model(inputs, False, 50)\n",
    "        # show_plot('generated', outputs.data.view(-1), True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_3.1]",
   "language": "python",
   "name": "conda-env-torch_3.1-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
