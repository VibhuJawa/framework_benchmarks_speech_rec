{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.multiprocessing as multiprocessing\n",
    "\n",
    "\n",
    "from torch import nn as nn\n",
    "from torch import optim\n",
    "from torch import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dir = 'data/pickles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_2_num_mapping = {}\n",
    "num_2_label_ar= []\n",
    "data = {\"x\":[],\"y\":[]}\n",
    "\n",
    "label_num = 0\n",
    "\n",
    "for label in os.listdir(input_dir):\n",
    "    if label == '.DS_Store':\n",
    "        continue\n",
    "    p_file = input_dir + label\n",
    "    label_2_num_mapping[label]=label_num\n",
    "    num_2_label_ar.append(label)\n",
    "\n",
    "    with open(p_file, 'rb') as f:\n",
    "        label_examples  = pickle.load(f)\n",
    "        # data dim = num_examples * time * frequency\n",
    "        label_ar = [np.swapaxes(np.array(example),0,1) for example in label_examples]\n",
    "        data['x'].extend(label_ar)\n",
    "        y_labels = [label_num for i in range(0,len(label_examples))]\n",
    "        data['y'].extend(y_labels)\n",
    "   \n",
    "    label_num+=1\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Samping function CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(n, X,Y):\n",
    "    \"\"\"\n",
    "    takes input and returns padded sample\n",
    "    n= num_samples\n",
    "    X = input_featurs_list\n",
    "    Y = label_list\n",
    "    \"\"\"\n",
    "    label_ids = np.random.randint(low = 0,high = len(X), size=n)\n",
    "    frequency = len(X[0][0])\n",
    "    sampled_X = [X[label_id] for label_id in label_ids]\n",
    "    sampled_y = [Y[label_id] for label_id in label_ids]\n",
    "    padded_X = []\n",
    "    \n",
    "    max_batch_len = max([len(x) for x in sampled_X])\n",
    "    for x in sampled_X:\n",
    "        padding_time_count = max_batch_len-len(x)\n",
    "        if padding_time_count!=0:\n",
    "            x_padded = np.zeros(shape = (max_batch_len,frequency))\n",
    "            x_padded[:x.shape[0],:x.shape[1]] = x\n",
    "            padded_X.append(x_padded)\n",
    "        else:\n",
    "            padded_X.append(x)\n",
    "        \n",
    "    return np.asarray(padded_X),np.asarray(sampled_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class WavesDatset(Dataset):\n",
    "    \"\"\"Loads the wavedataset\"\"\"\n",
    "\n",
    "    def __init__(self, data, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['x'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      \n",
    "        sample = {'x': data['x'][idx], 'label': data['y'][idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        _inputs, label = sample['x'], sample['label']\n",
    "        return {'x': torch.from_numpy(_inputs).float(),\n",
    "                'label': torch.from_numpy(np.array(label))\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "waves_dataset = WavesDatset(data,ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([44, 20]) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# cuda check\n",
    "for i in range(len(waves_dataset)):\n",
    "    sample = waves_dataset[i]\n",
    "    print(i, sample['x'].shape, sample['label'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wave_dataloader = DataLoader(waves_dataset, batch_size=4,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word_Predictor_RNN(nn.Module):\n",
    "    def __init__(self, input_freq=20,hidden_size=256,linear_output_size = 128, n_categories=30):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_freq, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.linear1 = nn.Linear(hidden_size, linear_output_size)\n",
    "        self.linear2 = nn.Linear(linear_output_size, n_categories)\n",
    "\n",
    "    def forward(self, x, initial_states):\n",
    "        # input now stores floats and has shape [length, batch_size, embedding_size]\n",
    "        self.rnn.flatten_parameters()\n",
    "        x, final_states = self.rnn(x, initial_states)  # TODO\n",
    "        x = final_states[0][0] * final_states[0][1]\n",
    "        self.rnn.flatten_parameters()\n",
    "        x = nn.functional.relu6(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model  Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_categories = len(label_2_num_mapping)\n",
    "batch_size = 64\n",
    "input_freq = 20\n",
    "hidden_size=256\n",
    "n_epochs = 10\n",
    "n_iters = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word_Predictor_RNN(input_freq=input_freq,hidden_size=hidden_size,n_categories=n_categories).cuda()\n",
    "wave_dataloader = DataLoader(waves_dataset, batch_size=batch_size,shuffle=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "print(len(wave_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number : 0 Batch_Number: 0 Avg Loss: 0.053203847259283066\n",
      "Epoch Number : 0 Batch_Number: 10 Avg Loss: 0.053178897974166\n",
      "Epoch Number : 0 Batch_Number: 20 Avg Loss: 0.05317046493291855\n",
      "Epoch Number : 0 Batch_Number: 30 Avg Loss: 0.0531472330612521\n",
      "Epoch Number : 0 Batch_Number: 40 Avg Loss: 0.053142107387141486\n",
      "Epoch Number : 0 Batch_Number: 50 Avg Loss: 0.17894366798755185\n",
      "Epoch Number : 1 Batch_Number: 0 Avg Loss: 0.05307523533701897\n",
      "Epoch Number : 1 Batch_Number: 10 Avg Loss: 0.05306529761715369\n",
      "Epoch Number : 1 Batch_Number: 20 Avg Loss: 0.053041055088951475\n",
      "Epoch Number : 1 Batch_Number: 30 Avg Loss: 0.05304625305917955\n",
      "Epoch Number : 1 Batch_Number: 40 Avg Loss: 0.053039426847202024\n",
      "Epoch Number : 1 Batch_Number: 50 Avg Loss: 0.1786532621019519\n",
      "1 173.11501097679138\n",
      "Epoch Number : 2 Batch_Number: 0 Avg Loss: 0.053084541112184525\n",
      "Epoch Number : 2 Batch_Number: 10 Avg Loss: 0.05302542752840302\n",
      "Epoch Number : 2 Batch_Number: 20 Avg Loss: 0.05301838864882787\n",
      "Epoch Number : 2 Batch_Number: 30 Avg Loss: 0.05301163429694791\n",
      "Epoch Number : 2 Batch_Number: 40 Avg Loss: 0.053001919806730455\n",
      "Epoch Number : 2 Batch_Number: 50 Avg Loss: 0.17856151449914073\n",
      "2 173.02610754966736\n",
      "Epoch Number : 3 Batch_Number: 0 Avg Loss: 0.05288735777139664\n",
      "Epoch Number : 3 Batch_Number: 10 Avg Loss: 0.05301072753288529\n",
      "Epoch Number : 3 Batch_Number: 20 Avg Loss: 0.05297332558603514\n",
      "Epoch Number : 3 Batch_Number: 30 Avg Loss: 0.052972892960233074\n",
      "Epoch Number : 3 Batch_Number: 40 Avg Loss: 0.05296614947842389\n",
      "Epoch Number : 3 Batch_Number: 50 Avg Loss: 0.1783918488013363\n",
      "3 172.86170148849487\n",
      "Epoch Number : 4 Batch_Number: 0 Avg Loss: 0.05295408517122269\n",
      "Epoch Number : 4 Batch_Number: 10 Avg Loss: 0.05299113623120568\n",
      "Epoch Number : 4 Batch_Number: 20 Avg Loss: 0.05292103439569473\n",
      "Epoch Number : 4 Batch_Number: 30 Avg Loss: 0.05291718869440017\n",
      "Epoch Number : 4 Batch_Number: 40 Avg Loss: 0.052892225181184166\n",
      "Epoch Number : 4 Batch_Number: 50 Avg Loss: 0.178108436773436\n",
      "4 172.58707523345947\n",
      "Epoch Number : 5 Batch_Number: 0 Avg Loss: 0.052739277482032776\n",
      "Epoch Number : 5 Batch_Number: 10 Avg Loss: 0.05280955440618775\n",
      "Epoch Number : 5 Batch_Number: 20 Avg Loss: 0.05280592523160435\n",
      "Epoch Number : 5 Batch_Number: 30 Avg Loss: 0.05279510588415207\n",
      "Epoch Number : 5 Batch_Number: 40 Avg Loss: 0.05278473465544421\n",
      "Epoch Number : 5 Batch_Number: 50 Avg Loss: 0.17780107069064713\n",
      "5 172.28923749923706\n",
      "Epoch Number : 6 Batch_Number: 0 Avg Loss: 0.05290110036730766\n",
      "Epoch Number : 6 Batch_Number: 10 Avg Loss: 0.05280457369305871\n",
      "Epoch Number : 6 Batch_Number: 20 Avg Loss: 0.05272271253523372\n",
      "Epoch Number : 6 Batch_Number: 30 Avg Loss: 0.05270472709690371\n",
      "Epoch Number : 6 Batch_Number: 40 Avg Loss: 0.05270134530416349\n",
      "Epoch Number : 6 Batch_Number: 50 Avg Loss: 0.1775112125034549\n",
      "6 172.00836491584778\n",
      "Epoch Number : 7 Batch_Number: 0 Avg Loss: 0.05268862098455429\n",
      "Epoch Number : 7 Batch_Number: 10 Avg Loss: 0.05257808078419079\n",
      "Epoch Number : 7 Batch_Number: 20 Avg Loss: 0.052617575795877544\n",
      "Epoch Number : 7 Batch_Number: 30 Avg Loss: 0.05264054358966889\n",
      "Epoch Number : 7 Batch_Number: 40 Avg Loss: 0.052624244333767306\n",
      "Epoch Number : 7 Batch_Number: 50 Avg Loss: 0.17714299998170205\n",
      "7 171.6515669822693\n",
      "Epoch Number : 8 Batch_Number: 0 Avg Loss: 0.05240435525774956\n",
      "Epoch Number : 8 Batch_Number: 10 Avg Loss: 0.05263026228005236\n",
      "Epoch Number : 8 Batch_Number: 20 Avg Loss: 0.052624951161089395\n",
      "Epoch Number : 8 Batch_Number: 30 Avg Loss: 0.05263850609621694\n",
      "Epoch Number : 8 Batch_Number: 40 Avg Loss: 0.05263206362724304\n",
      "Epoch Number : 8 Batch_Number: 50 Avg Loss: 0.17724120284750736\n",
      "8 171.74672555923462\n",
      "Epoch Number : 9 Batch_Number: 0 Avg Loss: 0.05259757488965988\n",
      "Epoch Number : 9 Batch_Number: 10 Avg Loss: 0.05247888680208813\n",
      "Epoch Number : 9 Batch_Number: 20 Avg Loss: 0.052548447712546305\n",
      "Epoch Number : 9 Batch_Number: 30 Avg Loss: 0.052552504645239924\n",
      "Epoch Number : 9 Batch_Number: 40 Avg Loss: 0.052543288233076656\n",
      "Epoch Number : 9 Batch_Number: 50 Avg Loss: 0.17698955757330076\n",
      "9 171.50288128852844\n"
     ]
    }
   ],
   "source": [
    "losses = np.zeros(n_epochs) # For plotting\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for i_batch, sample_batched in enumerate(wave_dataloader):\n",
    "        \n",
    "        _inputs,_labels = sample_batch(50,data['x'],data['y'])\n",
    "        inputs = torch.autograd.Variable(torch.from_numpy(_inputs).float())\n",
    "        targets = torch.autograd.Variable(torch.from_numpy(_labels))\n",
    "\n",
    "        inputs = torch.autograd.Variable(sample_batched['x'].cuda())\n",
    "        targets = torch.autograd.Variable(sample_batched['label'].cuda())\n",
    "        outputs = model(inputs, None)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses[epoch] += loss.item()\n",
    "        if i_batch%10==0:\n",
    "            curr_avg_loss = losses[epoch]/((i_batch+1)* len(inputs))\n",
    "            print(\"Epoch Number : {} Batch_Number: {} Avg Loss: {}\".format(epoch,i_batch,curr_avg_loss))\n",
    "        \n",
    "\n",
    "    if epoch > 0:\n",
    "        print(epoch, losses.data[epoch])\n",
    "\n",
    "    # Use some plotting library\n",
    "    # if epoch % 10 == 0:\n",
    "        # show_plot('inputs', _inputs, True)\n",
    "        # show_plot('outputs', outputs.data.view(-1), True)\n",
    "        # show_plot('losses', losses[:epoch] / n_iters)\n",
    "\n",
    "        # Generate a test\n",
    "        # outputs, hidden = model(inputs, False, 50)\n",
    "        # show_plot('generated', outputs.data.view(-1), True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_torch",
   "language": "python",
   "name": "pr1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
